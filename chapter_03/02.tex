%! TEX root = ./main.tex
\begin{exercise}[]{$\epsilon$-Greedy with Offline Oracles}
\end{exercise}
In Proposition 8, we analyzed the $\epsilon$-Greedy contextual bandit algorithm assuming access to an online regression oracle.
Because we appeal to online learning, this algorithm was able to handle adversarial contexts $x^1,\ldots,x^T$. 
In the present problem, we will modify the $\epsilon$-greedy algorithm and proof to show that if contexts are stochastic
(that is $x^t \sim \DD$ $\forall t,$ where $\DD$ is a fixed distribution), $\epsilon$-greedy works even if we use an \textit{offline oracle}.
We consider the following variant of $\epsilon$-greedy. 
The algorithm proceeds in epochs $m = 0,1,\ldots$ of doubling size
\[\{2\}, \{3,4\},\{5\ldots8\},\ldots\{2^m+1,2^{m+1}\},\ldots,\{T/2+1,T\};\]
we assume without loss of generality that $T$ is a power of 2, and that an arbitrary decision is made on round $t = 1$.
At the end of each epoch $m-1$, the offline oracle is invoked with the data from the epoch, producing an estimated model $\wh{f}^m.$
This model is used for the greedy step in the next epoch $m$. In other words, for any round $t \in [2^m +1,2^{m+1}]$ of epoch $m$,
the algorithm observes $x^t \sim \DD$, chooses an action $\pi^t \sim \text{unif}[A]$ with probability $\epsilon$ and chooses the greedy action 
\[ \pi^t = \arg \max_{\pi \in [A]}\wh f^m(x^t,\pi)\]
with probability $1-\epsilon$. Subsequently, the reward $r^t$ is observed.

1. Prove that for any $T \in  \NN$ and $\delta > 0$, by setting $\epsilon$ appropriately, this method ensures that
with probability at least $1-\delta$,
\[ \Reg \lesssim A^{1/3}T^{1/3}\left(\sum_{m=1}^{\log_2 T} 2^{m/2}\EstSq^{\text{off}}(\F,2^{m-1},\delta/m^2)^{1/2}\right)^{2/3}\]

2. Recall that for a finite class, 
ERM achieves $\EstSq^{\text{off}}(\F,T,\delta) \lesssim \log(|\F|/\delta).$ Show that with 
this choice, the above upper bound matches that in Proposition 8, up to logarithmic in $T$ factors.


\begin{solution}[TODO]
\end{solution}
